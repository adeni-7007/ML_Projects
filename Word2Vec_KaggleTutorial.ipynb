{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d706a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10987d",
   "metadata": {},
   "source": [
    "Read data from files, Verify the number of reviews that were read (100,000 in total):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f365ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv(\"/Users/alenadenisova/Desktop/Linguists/word2vec-nlp-tutorial/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"/Users/alenadenisova/Desktop/Linguists/word2vec-nlp-tutorial/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"/Users/alenadenisova/Desktop/Linguists/word2vec-nlp-tutorial/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74409ef",
   "metadata": {},
   "source": [
    "Import various modules for string cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc24108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ed3c5",
   "metadata": {},
   "source": [
    "Function to convert a document to a sequence of words,optionally removing stop words. Returns a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "977b874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    review_text = BeautifulSoup(review).get_text()     # 1. Remove HTML\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text) # 2. Remove non-letters\n",
    "    words = review_text.lower().split()                # 3. Convert words to lower case and split them\n",
    "    if remove_stopwords:                               # 4. Optionally remove stop words (false by default)\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)                                      # 5. Return a list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933be361",
   "metadata": {},
   "source": [
    "Download the punkt tokenizer for sentence splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a266b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "#nltk.download() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61ac32",
   "metadata": {},
   "source": [
    "Load the punkt tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2713f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e97ae",
   "metadata": {},
   "source": [
    "Define a function to split a review into parsed sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f82ab430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(review.strip())  # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    sentences = []                                      # 2. Loop over each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:                        # If a sentence is empty, skip it\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "            remove_stopwords ))                        # Otherwise, call review_to_wordlist to get a list of words\n",
    "    \n",
    "    return sentences                             # 3. Return the list of sentences(each sentence is a list of words,\n",
    "                                                        # so this returns a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5c8384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alenadenisova/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/alenadenisova/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8704d2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794002\n"
     ]
    }
   ],
   "source": [
    "# Check how many sentences we have in total - should be around 850,000+\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca12fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "879d669c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa41b5",
   "metadata": {},
   "source": [
    "Import the built-in logging module and configure it so that Word2Vec creates nice output messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea2f9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272abcc9",
   "metadata": {},
   "source": [
    "Set values for various parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3297b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64fe94",
   "metadata": {},
   "source": [
    "Initialize and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9bfa6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 10:45:23,652 : INFO : collecting all words and their counts\n",
      "2024-04-15 10:45:23,652 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-04-15 10:45:23,685 : INFO : PROGRESS: at sentence #10000, processed 225908 words, keeping 17776 word types\n",
      "2024-04-15 10:45:23,711 : INFO : PROGRESS: at sentence #20000, processed 452097 words, keeping 24953 word types\n",
      "2024-04-15 10:45:23,742 : INFO : PROGRESS: at sentence #30000, processed 671984 words, keeping 30044 word types\n",
      "2024-04-15 10:45:23,764 : INFO : PROGRESS: at sentence #40000, processed 898757 words, keeping 34358 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 10:45:23,785 : INFO : PROGRESS: at sentence #50000, processed 1122100 words, keeping 37824 word types\n",
      "2024-04-15 10:45:23,813 : INFO : PROGRESS: at sentence #60000, processed 1341946 words, keeping 40782 word types\n",
      "2024-04-15 10:45:23,834 : INFO : PROGRESS: at sentence #70000, processed 1566792 words, keeping 43383 word types\n",
      "2024-04-15 10:45:23,857 : INFO : PROGRESS: at sentence #80000, processed 1785875 words, keeping 45762 word types\n",
      "2024-04-15 10:45:23,879 : INFO : PROGRESS: at sentence #90000, processed 2010133 words, keeping 48195 word types\n",
      "2024-04-15 10:45:23,900 : INFO : PROGRESS: at sentence #100000, processed 2231529 words, keeping 50243 word types\n",
      "2024-04-15 10:45:23,921 : INFO : PROGRESS: at sentence #110000, processed 2451527 words, keeping 52128 word types\n",
      "2024-04-15 10:45:23,942 : INFO : PROGRESS: at sentence #120000, processed 2674780 words, keeping 54171 word types\n",
      "2024-04-15 10:45:23,963 : INFO : PROGRESS: at sentence #130000, processed 2899962 words, keeping 55883 word types\n",
      "2024-04-15 10:45:23,983 : INFO : PROGRESS: at sentence #140000, processed 3117846 words, keeping 57440 word types\n",
      "2024-04-15 10:45:24,004 : INFO : PROGRESS: at sentence #150000, processed 3343822 words, keeping 59128 word types\n",
      "2024-04-15 10:45:24,023 : INFO : PROGRESS: at sentence #160000, processed 3566494 words, keeping 60670 word types\n",
      "2024-04-15 10:45:24,043 : INFO : PROGRESS: at sentence #170000, processed 3789790 words, keeping 62146 word types\n",
      "2024-04-15 10:45:24,062 : INFO : PROGRESS: at sentence #180000, processed 4012439 words, keeping 63562 word types\n",
      "2024-04-15 10:45:24,082 : INFO : PROGRESS: at sentence #190000, processed 4238419 words, keeping 64874 word types\n",
      "2024-04-15 10:45:24,101 : INFO : PROGRESS: at sentence #200000, processed 4461934 words, keeping 66163 word types\n",
      "2024-04-15 10:45:24,120 : INFO : PROGRESS: at sentence #210000, processed 4685064 words, keeping 67472 word types\n",
      "2024-04-15 10:45:24,140 : INFO : PROGRESS: at sentence #220000, processed 4910487 words, keeping 68775 word types\n",
      "2024-04-15 10:45:24,160 : INFO : PROGRESS: at sentence #230000, processed 5132365 words, keeping 70035 word types\n",
      "2024-04-15 10:45:24,178 : INFO : PROGRESS: at sentence #240000, processed 5358801 words, keeping 71249 word types\n",
      "2024-04-15 10:45:24,195 : INFO : PROGRESS: at sentence #250000, processed 5571814 words, keeping 72424 word types\n",
      "2024-04-15 10:45:24,215 : INFO : PROGRESS: at sentence #260000, processed 5794438 words, keeping 73561 word types\n",
      "2024-04-15 10:45:24,234 : INFO : PROGRESS: at sentence #270000, processed 6016838 words, keeping 74906 word types\n",
      "2024-04-15 10:45:24,254 : INFO : PROGRESS: at sentence #280000, processed 6242604 words, keeping 76507 word types\n",
      "2024-04-15 10:45:24,274 : INFO : PROGRESS: at sentence #290000, processed 6467629 words, keeping 77947 word types\n",
      "2024-04-15 10:45:24,293 : INFO : PROGRESS: at sentence #300000, processed 6693272 words, keeping 79281 word types\n",
      "2024-04-15 10:45:24,313 : INFO : PROGRESS: at sentence #310000, processed 6918208 words, keeping 80610 word types\n",
      "2024-04-15 10:45:24,332 : INFO : PROGRESS: at sentence #320000, processed 7144010 words, keeping 81901 word types\n",
      "2024-04-15 10:45:24,354 : INFO : PROGRESS: at sentence #330000, processed 7367098 words, keeping 83148 word types\n",
      "2024-04-15 10:45:24,377 : INFO : PROGRESS: at sentence #340000, processed 7595248 words, keeping 84374 word types\n",
      "2024-04-15 10:45:24,405 : INFO : PROGRESS: at sentence #350000, processed 7820296 words, keeping 85555 word types\n",
      "2024-04-15 10:45:24,426 : INFO : PROGRESS: at sentence #360000, processed 8039653 words, keeping 86715 word types\n",
      "2024-04-15 10:45:24,462 : INFO : PROGRESS: at sentence #370000, processed 8269652 words, keeping 87823 word types\n",
      "2024-04-15 10:45:24,494 : INFO : PROGRESS: at sentence #380000, processed 8494009 words, keeping 88979 word types\n",
      "2024-04-15 10:45:24,523 : INFO : PROGRESS: at sentence #390000, processed 8723852 words, keeping 89972 word types\n",
      "2024-04-15 10:45:24,546 : INFO : PROGRESS: at sentence #400000, processed 8946001 words, keeping 91040 word types\n",
      "2024-04-15 10:45:24,568 : INFO : PROGRESS: at sentence #410000, processed 9167300 words, keeping 91974 word types\n",
      "2024-04-15 10:45:24,592 : INFO : PROGRESS: at sentence #420000, processed 9391149 words, keeping 93021 word types\n",
      "2024-04-15 10:45:24,619 : INFO : PROGRESS: at sentence #430000, processed 9618676 words, keeping 94019 word types\n",
      "2024-04-15 10:45:24,642 : INFO : PROGRESS: at sentence #440000, processed 9842880 words, keeping 95040 word types\n",
      "2024-04-15 10:45:24,670 : INFO : PROGRESS: at sentence #450000, processed 10070475 words, keeping 96145 word types\n",
      "2024-04-15 10:45:24,698 : INFO : PROGRESS: at sentence #460000, processed 10302379 words, keeping 97190 word types\n",
      "2024-04-15 10:45:24,721 : INFO : PROGRESS: at sentence #470000, processed 10530535 words, keeping 98075 word types\n",
      "2024-04-15 10:45:24,746 : INFO : PROGRESS: at sentence #480000, processed 10752969 words, keeping 98999 word types\n",
      "2024-04-15 10:45:24,769 : INFO : PROGRESS: at sentence #490000, processed 10977625 words, keeping 99967 word types\n",
      "2024-04-15 10:45:24,792 : INFO : PROGRESS: at sentence #500000, processed 11199925 words, keeping 100909 word types\n",
      "2024-04-15 10:45:24,822 : INFO : PROGRESS: at sentence #510000, processed 11425531 words, keeping 101772 word types\n",
      "2024-04-15 10:45:24,850 : INFO : PROGRESS: at sentence #520000, processed 11649706 words, keeping 102693 word types\n",
      "2024-04-15 10:45:24,872 : INFO : PROGRESS: at sentence #530000, processed 11874336 words, keeping 103507 word types\n",
      "2024-04-15 10:45:24,899 : INFO : PROGRESS: at sentence #540000, processed 12097597 words, keeping 104375 word types\n",
      "2024-04-15 10:45:24,928 : INFO : PROGRESS: at sentence #550000, processed 12324470 words, keeping 105242 word types\n",
      "2024-04-15 10:45:24,949 : INFO : PROGRESS: at sentence #560000, processed 12547431 words, keeping 106061 word types\n",
      "2024-04-15 10:45:24,977 : INFO : PROGRESS: at sentence #570000, processed 12774838 words, keeping 106893 word types\n",
      "2024-04-15 10:45:24,999 : INFO : PROGRESS: at sentence #580000, processed 12997866 words, keeping 107757 word types\n",
      "2024-04-15 10:45:25,022 : INFO : PROGRESS: at sentence #590000, processed 13222576 words, keeping 108592 word types\n",
      "2024-04-15 10:45:25,050 : INFO : PROGRESS: at sentence #600000, processed 13444789 words, keeping 109294 word types\n",
      "2024-04-15 10:45:25,071 : INFO : PROGRESS: at sentence #610000, processed 13667913 words, keeping 110174 word types\n",
      "2024-04-15 10:45:25,104 : INFO : PROGRESS: at sentence #620000, processed 13893899 words, keeping 110929 word types\n",
      "2024-04-15 10:45:25,126 : INFO : PROGRESS: at sentence #630000, processed 14116867 words, keeping 111712 word types\n",
      "2024-04-15 10:45:25,151 : INFO : PROGRESS: at sentence #640000, processed 14339687 words, keeping 112538 word types\n",
      "2024-04-15 10:45:25,180 : INFO : PROGRESS: at sentence #650000, processed 14564580 words, keeping 113286 word types\n",
      "2024-04-15 10:45:25,210 : INFO : PROGRESS: at sentence #660000, processed 14787776 words, keeping 114037 word types\n",
      "2024-04-15 10:45:25,241 : INFO : PROGRESS: at sentence #670000, processed 15011688 words, keeping 114731 word types\n",
      "2024-04-15 10:45:25,265 : INFO : PROGRESS: at sentence #680000, processed 15236706 words, keeping 115440 word types\n",
      "2024-04-15 10:45:25,291 : INFO : PROGRESS: at sentence #690000, processed 15460838 words, keeping 116257 word types\n",
      "2024-04-15 10:45:25,318 : INFO : PROGRESS: at sentence #700000, processed 15690541 words, keeping 117033 word types\n",
      "2024-04-15 10:45:25,341 : INFO : PROGRESS: at sentence #710000, processed 15912096 words, keeping 117691 word types\n",
      "2024-04-15 10:45:25,363 : INFO : PROGRESS: at sentence #720000, processed 16137798 words, keeping 118362 word types\n",
      "2024-04-15 10:45:25,395 : INFO : PROGRESS: at sentence #730000, processed 16364713 words, keeping 119075 word types\n",
      "2024-04-15 10:45:25,416 : INFO : PROGRESS: at sentence #740000, processed 16584901 words, keeping 119762 word types\n",
      "2024-04-15 10:45:25,439 : INFO : PROGRESS: at sentence #750000, processed 16805675 words, keeping 120418 word types\n",
      "2024-04-15 10:45:25,460 : INFO : PROGRESS: at sentence #760000, processed 17021917 words, keeping 121065 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 10:45:25,488 : INFO : PROGRESS: at sentence #770000, processed 17252420 words, keeping 121803 word types\n",
      "2024-04-15 10:45:25,509 : INFO : PROGRESS: at sentence #780000, processed 17481011 words, keeping 122484 word types\n",
      "2024-04-15 10:45:25,545 : INFO : PROGRESS: at sentence #790000, processed 17710319 words, keeping 123190 word types\n",
      "2024-04-15 10:45:25,554 : INFO : collected 123504 word types from a corpus of 17798270 raw words and 794002 sentences\n",
      "2024-04-15 10:45:25,555 : INFO : Creating a fresh vocabulary\n",
      "2024-04-15 10:45:25,620 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16490 unique words (13.35% of original 123504, drops 107014)', 'datetime': '2024-04-15T10:45:25.619966', 'gensim': '4.3.2', 'python': '3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]', 'platform': 'macOS-12.4-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-04-15 10:45:25,620 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 17239125 word corpus (96.86% of original 17798270, drops 559145)', 'datetime': '2024-04-15T10:45:25.620513', 'gensim': '4.3.2', 'python': '3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]', 'platform': 'macOS-12.4-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-04-15 10:45:25,648 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2024-04-15 10:45:25,651 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2024-04-15 10:45:25,651 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12749798.434354488 word corpus (74.0%% of prior 17239125)', 'datetime': '2024-04-15T10:45:25.651383', 'gensim': '4.3.2', 'python': '3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]', 'platform': 'macOS-12.4-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-04-15 10:45:25,699 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2024-04-15 10:45:25,700 : INFO : resetting layer weights\n",
      "2024-04-15 10:45:25,722 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-04-15T10:45:25.722214', 'gensim': '4.3.2', 'python': '3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]', 'platform': 'macOS-12.4-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2024-04-15 10:45:25,722 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-04-15T10:45:25.722620', 'gensim': '4.3.2', 'python': '3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]', 'platform': 'macOS-12.4-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-04-15 10:45:26,728 : INFO : EPOCH 0 - PROGRESS: at 15.28% examples, 1932000 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:27,736 : INFO : EPOCH 0 - PROGRESS: at 31.06% examples, 1959296 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:28,740 : INFO : EPOCH 0 - PROGRESS: at 46.52% examples, 1960744 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:29,745 : INFO : EPOCH 0 - PROGRESS: at 61.90% examples, 1961561 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:30,747 : INFO : EPOCH 0 - PROGRESS: at 76.59% examples, 1943423 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-15 10:45:31,747 : INFO : EPOCH 0 - PROGRESS: at 91.98% examples, 1947103 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-15 10:45:32,264 : INFO : EPOCH 0: training on 17798270 raw words (12749249 effective words) took 6.5s, 1949745 effective words/s\n",
      "2024-04-15 10:45:33,272 : INFO : EPOCH 1 - PROGRESS: at 13.93% examples, 1758656 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:34,274 : INFO : EPOCH 1 - PROGRESS: at 27.34% examples, 1728045 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:35,277 : INFO : EPOCH 1 - PROGRESS: at 40.37% examples, 1703509 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:36,277 : INFO : EPOCH 1 - PROGRESS: at 53.29% examples, 1690500 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:37,283 : INFO : EPOCH 1 - PROGRESS: at 66.61% examples, 1692264 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:38,284 : INFO : EPOCH 1 - PROGRESS: at 81.20% examples, 1719701 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-15 10:45:39,286 : INFO : EPOCH 1 - PROGRESS: at 92.73% examples, 1684276 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:39,815 : INFO : EPOCH 1: training on 17798270 raw words (12749986 effective words) took 7.5s, 1689128 effective words/s\n",
      "2024-04-15 10:45:40,826 : INFO : EPOCH 2 - PROGRESS: at 15.28% examples, 1923595 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:41,830 : INFO : EPOCH 2 - PROGRESS: at 28.67% examples, 1809441 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:42,830 : INFO : EPOCH 2 - PROGRESS: at 42.16% examples, 1777754 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:43,831 : INFO : EPOCH 2 - PROGRESS: at 57.39% examples, 1820764 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-15 10:45:44,832 : INFO : EPOCH 2 - PROGRESS: at 71.92% examples, 1828066 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:45,838 : INFO : EPOCH 2 - PROGRESS: at 85.05% examples, 1800882 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:46,839 : INFO : EPOCH 2 - PROGRESS: at 99.94% examples, 1815076 words/s, in_qsize 1, out_qsize 1\n",
      "2024-04-15 10:45:46,839 : INFO : EPOCH 2: training on 17798270 raw words (12749545 effective words) took 7.0s, 1815826 effective words/s\n",
      "2024-04-15 10:45:47,845 : INFO : EPOCH 3 - PROGRESS: at 14.77% examples, 1868296 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-15 10:45:48,850 : INFO : EPOCH 3 - PROGRESS: at 30.12% examples, 1904465 words/s, in_qsize 8, out_qsize 1\n",
      "2024-04-15 10:45:49,859 : INFO : EPOCH 3 - PROGRESS: at 45.68% examples, 1924475 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:50,869 : INFO : EPOCH 3 - PROGRESS: at 61.28% examples, 1938778 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:51,869 : INFO : EPOCH 3 - PROGRESS: at 76.76% examples, 1945528 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:52,874 : INFO : EPOCH 3 - PROGRESS: at 92.21% examples, 1948935 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:53,368 : INFO : EPOCH 3: training on 17798270 raw words (12749903 effective words) took 6.5s, 1953549 effective words/s\n",
      "2024-04-15 10:45:54,372 : INFO : EPOCH 4 - PROGRESS: at 15.56% examples, 1969977 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:55,383 : INFO : EPOCH 4 - PROGRESS: at 28.90% examples, 1822075 words/s, in_qsize 8, out_qsize 1\n",
      "2024-04-15 10:45:56,391 : INFO : EPOCH 4 - PROGRESS: at 44.41% examples, 1867208 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:57,393 : INFO : EPOCH 4 - PROGRESS: at 59.81% examples, 1894319 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:58,398 : INFO : EPOCH 4 - PROGRESS: at 75.29% examples, 1908432 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:45:59,406 : INFO : EPOCH 4 - PROGRESS: at 90.59% examples, 1913140 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-15 10:46:00,019 : INFO : EPOCH 4: training on 17798270 raw words (12748101 effective words) took 6.6s, 1917411 effective words/s\n",
      "2024-04-15 10:46:00,020 : INFO : Word2Vec lifecycle event {'msg': 'training on 88991350 raw words (63746784 effective words) took 34.3s, 1858667 effective words/s', 'datetime': '2024-04-15T10:46:00.020042', 'gensim': '4.3.2', 'python': '3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]', 'platform': 'macOS-12.4-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-04-15 10:46:00,020 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16490, vector_size=300, alpha=0.025>', 'datetime': '2024-04-15T10:46:00.020372', 'gensim': '4.3.2', 'python': '3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]', 'platform': 'macOS-12.4-arm64-arm-64bit', 'event': 'created'}\n",
      "/var/folders/4s/947pzpws41b40dzj46z_t3wc0000gn/T/ipykernel_37601/2031041940.py:8: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n",
      "2024-04-15 10:46:00,027 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "2024-04-15 10:46:00,028 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-04-15T10:46:00.028877', 'gensim': '4.3.2', 'python': '3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]', 'platform': 'macOS-12.4-arm64-arm-64bit', 'event': 'saving'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 10:46:00,029 : INFO : not storing attribute cum_table\n",
      "2024-04-15 10:46:00,077 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            vector_size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling init_sims will make the model more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1199d6d",
   "metadata": {},
   "source": [
    "Exploring the Model Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "043a5f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d08c3cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54f47c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6165322065353394),\n",
       " ('lady', 0.5824472904205322),\n",
       " ('lad', 0.562603235244751),\n",
       " ('men', 0.5167946219444275),\n",
       " ('monk', 0.5155864953994751),\n",
       " ('guy', 0.5074439644813538),\n",
       " ('soldier', 0.5072890520095825),\n",
       " ('millionaire', 0.5027772188186646),\n",
       " ('businessman', 0.5011990070343018),\n",
       " ('person', 0.4968433380126953)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b827abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue', 0.6756093502044678),\n",
       " ('yellow', 0.6158223152160645),\n",
       " ('soylent', 0.5916862487792969),\n",
       " ('red', 0.5511486530303955),\n",
       " ('tinted', 0.5438907146453857),\n",
       " ('lighted', 0.5380668640136719),\n",
       " ('fur', 0.532646119594574),\n",
       " ('flashlight', 0.5303142070770264),\n",
       " ('colored', 0.521892249584198),\n",
       " ('silver', 0.5187628269195557)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86151460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.6954634189605713),\n",
       " ('bad', 0.6345804929733276),\n",
       " ('great', 0.6118682622909546),\n",
       " ('nice', 0.5847127437591553),\n",
       " ('lousy', 0.5817535519599915),\n",
       " ('fine', 0.563975989818573),\n",
       " ('passable', 0.5555528998374939),\n",
       " ('mediocre', 0.55186927318573),\n",
       " ('cool', 0.5485385656356812),\n",
       " ('watchable', 0.5241504907608032)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfa5d8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7726731300354004),\n",
       " ('atrocious', 0.7428853511810303),\n",
       " ('dreadful', 0.7237646579742432),\n",
       " ('horrible', 0.7184943556785583),\n",
       " ('abysmal', 0.6965259909629822),\n",
       " ('horrendous', 0.6892979145050049),\n",
       " ('appalling', 0.6839365363121033),\n",
       " ('horrid', 0.6575130224227905),\n",
       " ('crappy', 0.610087513923645),\n",
       " ('amateurish', 0.6070759892463684)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a2e199e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 10:46:00,202 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2024-04-15 10:46:00,219 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2024-04-15 10:46:00,220 : INFO : setting ignored attribute cum_table to None\n",
      "2024-04-15 10:46:00,302 : INFO : Word2Vec lifecycle event {'fname': '300features_40minwords_10context', 'datetime': '2024-04-15T10:46:00.302853', 'gensim': '4.3.2', 'python': '3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]', 'platform': 'macOS-12.4-arm64-arm-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# Load the model that we created\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ddb3cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.vectors) # vectors вместо syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe660430",
   "metadata": {},
   "source": [
    "KeyedVectors.load_word2vec_format instead of  Word2Vec.load_word2vec_format\n",
    "\n",
    "word2vec_model.wv.save_word2vec_format instead of  word2vec_model.save_word2vec_format\n",
    "\n",
    "model.wv.syn0norm instead of  model.syn0norm\n",
    "\n",
    "model.wv.syn0 instead of  model.syn0\n",
    "\n",
    "model.wv.vocab instead of model.vocab\n",
    "\n",
    "model.wv.index2word instead of  model.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25d99c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81b88348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0061881 ,  0.02481586,  0.00612303,  0.07100722, -0.00914465,\n",
       "        0.03688773,  0.01628157,  0.05933855,  0.0604945 , -0.07720757,\n",
       "        0.02967836,  0.07661617,  0.00264418, -0.04631488, -0.05260531,\n",
       "        0.00100371,  0.02289619, -0.13530129, -0.05669887,  0.01918228,\n",
       "       -0.05199756,  0.00798491,  0.01625661,  0.0367662 ,  0.14335768,\n",
       "       -0.00121609, -0.0162967 , -0.02419809, -0.04577519,  0.05443172,\n",
       "        0.04023934, -0.02037233, -0.01740628, -0.04092016, -0.06050093,\n",
       "       -0.00884334, -0.01062836, -0.05201737, -0.00326537,  0.06021222,\n",
       "       -0.04865903,  0.04232333,  0.08118019,  0.0285969 , -0.11885691,\n",
       "       -0.02770385,  0.11113984,  0.0771514 ,  0.01775114,  0.02233298,\n",
       "        0.11325468,  0.06049047, -0.04097607, -0.02647992, -0.0054828 ,\n",
       "        0.01072516,  0.0730335 , -0.0010708 , -0.03390911, -0.0086191 ,\n",
       "       -0.14693284,  0.09438164,  0.08965211,  0.05836939,  0.02989194,\n",
       "        0.01673451, -0.10435382, -0.02144188,  0.01198142, -0.06500129,\n",
       "        0.02198704,  0.10053432,  0.02944489, -0.00237288, -0.06176569,\n",
       "       -0.03496229, -0.06069201,  0.01636249, -0.051832  ,  0.0290908 ,\n",
       "       -0.00535358,  0.00832751, -0.0266109 ,  0.16566391,  0.0113215 ,\n",
       "        0.05040337, -0.02257375,  0.03093017,  0.02548861, -0.0097364 ,\n",
       "       -0.00717034,  0.006935  , -0.04501983,  0.0360866 ,  0.09853241,\n",
       "       -0.00109352,  0.07126521, -0.06683604, -0.121826  ,  0.10774613,\n",
       "        0.00329003,  0.10304672,  0.09581649, -0.08095062,  0.16113912,\n",
       "       -0.00486724, -0.04590366, -0.05185772, -0.03070164,  0.04980734,\n",
       "       -0.03917953,  0.00953077,  0.05895405,  0.00491644,  0.03614123,\n",
       "        0.10449699,  0.03854235,  0.0592705 , -0.00353015, -0.04364865,\n",
       "        0.011552  ,  0.0129466 ,  0.06439065,  0.00663922, -0.04518822,\n",
       "        0.06166495,  0.06002201, -0.09859724,  0.02721667,  0.06887489,\n",
       "        0.06497649,  0.0902796 ,  0.00512071, -0.09411144,  0.10053497,\n",
       "       -0.01892231, -0.03561984,  0.04768717,  0.04760291,  0.07056694,\n",
       "        0.01193191, -0.06589693, -0.08052915,  0.0036845 ,  0.03296661,\n",
       "       -0.00440713, -0.06183241, -0.08237994,  0.05592391,  0.08875629,\n",
       "        0.0789797 ,  0.02092867, -0.08972649,  0.06571984,  0.01554936,\n",
       "       -0.02106343,  0.00475846, -0.00907776, -0.05565501,  0.04729636,\n",
       "        0.02360421, -0.07262751, -0.06171834,  0.06769362, -0.02279436,\n",
       "       -0.03455747,  0.11464765, -0.04869377,  0.06187423,  0.01773787,\n",
       "       -0.03288186,  0.05361316, -0.03191855,  0.05432272, -0.07389377,\n",
       "       -0.04917918,  0.01920789, -0.0889044 , -0.03955826, -0.02379802,\n",
       "        0.06838541,  0.12190055, -0.0091456 , -0.11209335,  0.0168145 ,\n",
       "       -0.04441936, -0.00924023, -0.00606809,  0.13468231, -0.04327599,\n",
       "        0.02011513, -0.08567703, -0.10305006,  0.08108445,  0.01685084,\n",
       "       -0.02164254,  0.0137386 , -0.04994154,  0.07772005, -0.01246203,\n",
       "       -0.10641227,  0.05116735, -0.00069538, -0.11782942, -0.03772822,\n",
       "       -0.00578524, -0.01390926,  0.14353298,  0.00151693,  0.004028  ,\n",
       "        0.00307455, -0.11054   ,  0.05439918, -0.03095713,  0.00497481,\n",
       "        0.03137725, -0.00317306, -0.18035366, -0.04490584,  0.02352864,\n",
       "        0.12737034,  0.05487389, -0.0394186 , -0.06153572, -0.03039298,\n",
       "       -0.09847989, -0.0138029 ,  0.01815502, -0.0269952 , -0.02875364,\n",
       "       -0.08866969,  0.02469552, -0.05911329, -0.00751558,  0.0570717 ,\n",
       "        0.0252763 , -0.00160183,  0.0181097 ,  0.00181687, -0.09915028,\n",
       "        0.05835191, -0.00599939,  0.00919086, -0.01748515,  0.00921312,\n",
       "        0.07795241,  0.02774014, -0.0053902 ,  0.02387688,  0.01588948,\n",
       "       -0.00148414,  0.03864598,  0.02941376, -0.01223838,  0.0334156 ,\n",
       "       -0.03045864, -0.00560328, -0.00481064, -0.03336435, -0.04770068,\n",
       "       -0.0018632 , -0.04278899,  0.0144967 ,  0.00547525, -0.0193773 ,\n",
       "       -0.02438188,  0.02444835, -0.00250777, -0.01254816,  0.00331895,\n",
       "       -0.03398013,  0.05070028, -0.04579158, -0.02628916,  0.01570033,\n",
       "       -0.03239968, -0.00735429, -0.08208524, -0.01561375, -0.00793583,\n",
       "        0.00940055,  0.01915517, -0.06504052, -0.02396607,  0.01762438,\n",
       "       -0.04830907,  0.03731683,  0.07981499,  0.0271587 ,  0.07107866,\n",
       "       -0.0454271 , -0.03146373,  0.17672426, -0.00820849,  0.07483658,\n",
       "        0.10693316, -0.0582132 , -0.12081058,  0.05082788, -0.01623974],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"flower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7b24aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv[\"flower\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c87610",
   "metadata": {},
   "source": [
    "From Words To Paragraphs, Attempt 1: Vector Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb0c2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21a856",
   "metadata": {},
   "source": [
    "Function to average all of the word vectors in a given paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d496cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")   # 1.Pre-initialize an empty numpy array (for speed)\n",
    "    nwords = 0.\n",
    "    \n",
    "    index2word_set = set(model.wv.index_to_key)    # 2.Index2word is a list that contains the names of the words in \n",
    "                                                  # the model's vocabulary. Convert it to a set, for speed \n",
    "    for word in words:                        # 3.Loop over each word in the review and, if it is in the model's \n",
    "        if word in index2word_set:                #vocaublary, add its feature vector to the total\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model.wv[word])\n",
    "   \n",
    "    featureVec = np.divide(featureVec,nwords) # Divide the result by the number of words to get the average\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "   \n",
    "    counter = 0                                                              # 1. Initialize a counter\n",
    "    \n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\") # 2. Preallocate a 2D numpy array, for speed\n",
    "    \n",
    "    for review in reviews:                                                    # 3. Loop through the reviews \n",
    "        if counter%1000 == 0:                                               # 3.1. Print a status message every 1000th review\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "            reviewFeatureVecs[counter] = makeFeatureVec(review, model,       # 3.2.Call the function that makes average feature vectors\n",
    "           num_features)\n",
    "        counter = counter + 1                                                 # 4. Increment the counter\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b710f1",
   "metadata": {},
   "source": [
    "Calculate average feature vectors for training and testing sets, using the functions we defined above. Notice that we now use stop word\n",
    "removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1044d658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alenadenisova/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "\n",
    "clean_test_reviews = []\n",
    "\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89c50c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n",
      "ready\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abde88d",
   "metadata": {},
   "source": [
    "From Words to Paragraphs, Attempt 2: Clustering (\"vector quantization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62c3b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01ac57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv.vectors # instead of model.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "#Setting \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an average of 5 words per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2832f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1337e1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alenadenisova/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "566e0eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  485.24791288375854 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1646ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.wv.index_to_key, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a2a7e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['kiddies', 'spectators', 'westerners']\n",
      "\n",
      "Cluster 1\n",
      "['posse']\n",
      "\n",
      "Cluster 2\n",
      "['janeane']\n",
      "\n",
      "Cluster 3\n",
      "['spirit', 'wrath', 'backbone', 'grapes']\n",
      "\n",
      "Cluster 4\n",
      "['scheming', 'stubborn', 'meek', 'reckless', 'conniving', 'treacherous', 'shrewd', 'calculating', 'sullen', 'headstrong', 'penniless']\n",
      "\n",
      "Cluster 5\n",
      "['prop', 'chained', 'burying', 'whipping', 'chatting', 'clinging', 'locking']\n",
      "\n",
      "Cluster 6\n",
      "['baker', 'francis', 'alexander', 'antonio', 'cole', 'freddie', 'bryan', 'houston', 'banderas', 'jacob', 'rudolph', 'chang', 'rene', 'caruso', 'rudolf', 'giovanni', 'klaus', 'luigi', 'whitman']\n",
      "\n",
      "Cluster 7\n",
      "['organized', 'awaiting', 'condemned', 'sacrificed', 'immune']\n",
      "\n",
      "Cluster 8\n",
      "['filled', 'abound', 'laced', 'interspersed', 'riddled', 'sprinkled', 'rife', 'peppered', 'littered']\n",
      "\n",
      "Cluster 9\n",
      "['release', 'public', 'industry', 'market', 'marketing', 'publicity', 'distribution', 'funding', 'promotion', 'distributor', 'distributors']\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in range(0,10):\n",
    "    print(\"\\nCluster %d\" % cluster)        # Print the cluster number  \n",
    "    words = []                            # Find all of the words for that cluster number, and print them out        \n",
    "    for i in range(0,len(list(word_centroid_map.values()))):\n",
    "        if( list(word_centroid_map.values())[i] == cluster ):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)\n",
    "    \n",
    "    #the_values = dict.values()\n",
    "#SUM = sum(list(the_values)[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b22aa7a",
   "metadata": {},
   "source": [
    "In python-3.X dict.values doesn't return a list object like how it used to perform in python-2.X. \n",
    "In python-3.x it returns a dict-value object which is a set-like object and uses a hash table for storing its items which is not suitable for indexing. This feature, in addition to supporting most of set attributes, is very optimized for operations like membership checking (using in operator).\n",
    "By assigning dict.values() to a list you are not converting it to a list; you are just storing it as dict_values (may be an object). To convert it write value_list=list(dict.values()). Now even while printing the value_list you will get the list elements and not dict_values(......).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913059b",
   "metadata": {},
   "source": [
    "now we have a cluster (or \"centroid\") assignment for each word, and we can define a function to convert reviews into bags-of-centroids. This works just like Bag of Words but uses semantically related clusters instead of individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72cc5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1   #The number of clusters is equal to the highest cluster\n",
    "                                                            #index in the word/centroid map\n",
    "\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" ) # Pre-allocate the bag of centroids vector (for speed)\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fad82863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3446accf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest and extract predictions \n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "559c9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.syn0 --> .wv.vectors\n",
    "#.index2word --> .wv.index_to_key\n",
    "#dictionary[i] --> list(dictionary.values())[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "beaece0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d39d9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}]'.format(results.best_params_))\n",
    "    \n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+-{}) for {}'.format(round(mean,3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f2a4802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;max_depth&#x27;: [16], &#x27;n_estimators&#x27;: [100, 250]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;max_depth&#x27;: [16], &#x27;n_estimators&#x27;: [100, 250]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [16], 'n_estimators': [100, 250]})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [100, 250],\n",
    "    'max_depth': [16]\n",
    "}\n",
    "cv = GridSearchCV(rf, parameters, cv = 3)\n",
    "cv.fit(train_centroids,train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f13274ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'max_depth': 16, 'n_estimators': 250}]\n",
      "0.836 (+-0.008) for {'max_depth': 16, 'n_estimators': 100}\n",
      "0.842 (+-0.007) for {'max_depth': 16, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da46a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=16, n_estimators=250)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=16, n_estimators=250)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=16, n_estimators=250)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c9289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
